<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[PROCJAM 2017] An emotion based framework for procedural music video generation | Broken Shadow Maps</title><meta name=keywords content><meta name=description content="For PROCJAM &ndash; a jam about procedural generation &ndash; I built a proof of concept for a music video generator based on compositional principles.
It leverages a simplified emotion representation, a set of music annotation tools based on such framework and a director/solver that figures out how to represent a specific set of assets and scenes in a spatio-temporal way that is aligned with the music.
It uses a very simple Monte Carlo planning scheme where every frame it generates thousands of alternative shots, evaluates a heuristic for shot quality and chooses the best one."><meta name=author content><link rel=canonical href=https://mmerchante.github.io/blog-public/publications/procjam/><link crossorigin=anonymous href=../../blog-public/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../blog-public/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://mmerchante.github.io/blog-public/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://mmerchante.github.io/blog-public/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://mmerchante.github.io/blog-public/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://mmerchante.github.io/blog-public/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://mmerchante.github.io/blog-public/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="[PROCJAM 2017] An emotion based framework for procedural music video generation"><meta property="og:description" content="For PROCJAM &ndash; a jam about procedural generation &ndash; I built a proof of concept for a music video generator based on compositional principles.
It leverages a simplified emotion representation, a set of music annotation tools based on such framework and a director/solver that figures out how to represent a specific set of assets and scenes in a spatio-temporal way that is aligned with the music.
It uses a very simple Monte Carlo planning scheme where every frame it generates thousands of alternative shots, evaluates a heuristic for shot quality and chooses the best one."><meta property="og:type" content="article"><meta property="og:url" content="https://mmerchante.github.io/blog-public/publications/procjam/"><meta property="article:section" content="publications"><meta property="article:published_time" content="2017-11-09T00:00:00+00:00"><meta property="article:modified_time" content="2017-11-09T00:00:00+00:00"><meta property="og:site_name" content="Broken Shadow Maps"><meta name=twitter:card content="summary"><meta name=twitter:title content="[PROCJAM 2017] An emotion based framework for procedural music video generation"><meta name=twitter:description content="For PROCJAM &ndash; a jam about procedural generation &ndash; I built a proof of concept for a music video generator based on compositional principles.
It leverages a simplified emotion representation, a set of music annotation tools based on such framework and a director/solver that figures out how to represent a specific set of assets and scenes in a spatio-temporal way that is aligned with the music.
It uses a very simple Monte Carlo planning scheme where every frame it generates thousands of alternative shots, evaluates a heuristic for shot quality and chooses the best one."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Publications","item":"https://mmerchante.github.io/blog-public/publications/"},{"@type":"ListItem","position":2,"name":"[PROCJAM 2017] An emotion based framework for procedural music video generation","item":"https://mmerchante.github.io/blog-public/publications/procjam/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[PROCJAM 2017] An emotion based framework for procedural music video generation","name":"[PROCJAM 2017] An emotion based framework for procedural music video generation","description":"For PROCJAM \u0026ndash; a jam about procedural generation \u0026ndash; I built a proof of concept for a music video generator based on compositional principles.\nIt leverages a simplified emotion representation, a set of music annotation tools based on such framework and a director/solver that figures out how to represent a specific set of assets and scenes in a spatio-temporal way that is aligned with the music.\nIt uses a very simple Monte Carlo planning scheme where every frame it generates thousands of alternative shots, evaluates a heuristic for shot quality and chooses the best one.","keywords":[],"articleBody":" For PROCJAM – a jam about procedural generation – I built a proof of concept for a music video generator based on compositional principles.\nIt leverages a simplified emotion representation, a set of music annotation tools based on such framework and a director/solver that figures out how to represent a specific set of assets and scenes in a spatio-temporal way that is aligned with the music.\nIt uses a very simple Monte Carlo planning scheme where every frame it generates thousands of alternative shots, evaluates a heuristic for shot quality and chooses the best one.\nUnfortunately, I had very little time to build this somewhat ambitious project, and although I haven’t gotten back to it (it’s still a sketch!), I think it has potential considering the recent surge in improved ML models. I believe the annotations can be trained and inferred very easily, and the director can still have hand written heuristics built on pattern matching from these annotations. Alternatively, the director model could also be a fully trained model, but it becomes harder to find and generate data to train such scheme.\nAn example track with annotated emotional events, and evaluated energy values\nThe heuristics used for the director are basically encoded principles of composition: it can vary from a simple rule of thirds to contrast, repetition (shape, position, color) and other associated concepts like Gestalt principles.\nYou can see more details in the talk, but please note it was a very early proof of concept and it needs a lot more work!\n","wordCount":"254","inLanguage":"en","datePublished":"2017-11-09T00:00:00Z","dateModified":"2017-11-09T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://mmerchante.github.io/blog-public/publications/procjam/"},"publisher":{"@type":"Organization","name":"Broken Shadow Maps","logo":{"@type":"ImageObject","url":"https://mmerchante.github.io/blog-public/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://mmerchante.github.io/blog-public/ accesskey=h title="Broken Shadow Maps (Alt + H)"><img src=https://mmerchante.github.io/apple-touch-icon.png alt aria-label=logo height=35>Broken Shadow Maps</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://mmerchante.github.io/blog-public/ title=Home><span>Home</span></a></li><li><a href=https://mmerchante.github.io/blog-public/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mmerchante.github.io/blog-public/projects/tech title=Technical><span>Technical</span></a></li><li><a href=https://mmerchante.github.io/blog-public/projects/art title=Art><span>Art</span></a></li><li><a href=https://mmerchante.github.io/blog-public/publications/ title=Publications/Talks><span>Publications/Talks</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>[PROCJAM 2017] An emotion based framework for procedural music video generation</h1><div class=post-meta><span title='2017-11-09 00:00:00 +0000 UTC'>November 9, 2017</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;254 words</div></header><div class=post-content><iframe src=https://www.youtube.com/embed/EBDkzq-WF1c width=800 height=360 frameborder=0 allowfullscreen></iframe><p>For PROCJAM &ndash; a jam about procedural generation &ndash; I built a <em>proof of concept</em> for a music video generator based on compositional principles.</p><p>It leverages a simplified emotion representation, a set of music annotation tools based on such framework and a director/solver that figures out how to represent a specific set of assets and scenes in a spatio-temporal way that is aligned with the music.</p><p>It uses a very simple Monte Carlo planning scheme where every frame it generates thousands of alternative shots, evaluates a heuristic for shot quality and chooses the best one.</p><p>Unfortunately, I had very little time to build this somewhat ambitious project, and although I haven&rsquo;t gotten back to it (it&rsquo;s still a sketch!), I think it has potential considering the recent surge in improved ML models. I believe the annotations can be trained and inferred very easily, and the director can still have hand written heuristics built on pattern matching from these annotations. Alternatively, the director model could also be a fully trained model, but it becomes harder to find and generate data to train such scheme.</p><p><img loading=lazy src=../../img/emotions/emotions_1.jpg alt>
<em>An example track with annotated emotional events, and evaluated energy values</em></p><p>The heuristics used for the director are basically encoded principles of composition: it can vary from a simple rule of thirds to contrast, repetition (shape, position, color) and other associated concepts like Gestalt principles.</p><p>You can see more details in the talk, but please note it was a very early proof of concept and it needs a lot more work!</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://mmerchante.github.io/blog-public/>Broken Shadow Maps</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>